# Multi-Model Pipeline Workflow using Argo Workflows
# Chains models together (e.g., OCR â†’ NLP) for complex processing

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: multi-model-pipeline
  namespace: ai-models
  labels:
    app: pipeline-orchestrator
spec:
  entrypoint: multi-model-pipeline
  
  # Input parameters
  arguments:
    parameters:
      - name: input-data
        description: "Input data for the pipeline"
      - name: pipeline-config
        description: "Pipeline configuration JSON"
        value: |
          {
            "models": [
              {
                "name": "ocr-model",
                "endpoint": "http://ocr-model-service:8000",
                "input_field": "image",
                "output_field": "text"
              },
              {
                "name": "bert-ner",
                "endpoint": "http://bert-ner-model-service:8000",
                "input_field": "text",
                "output_field": "entities"
              }
            ]
          }
      - name: timeout
        value: "300"
        description: "Pipeline timeout in seconds"
  
  # Volume templates for sharing data between steps
  volumeClaimTemplates:
    - metadata:
        name: pipeline-data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi
  
  templates:
    # Main pipeline template
    - name: multi-model-pipeline
      dag:
        tasks:
          # Initialize pipeline
          - name: initialize
            template: initialize-pipeline
            arguments:
              parameters:
                - name: input-data
                  value: "{{workflow.parameters.input-data}}"
                - name: pipeline-config
                  value: "{{workflow.parameters.pipeline-config}}"
          
          # OCR processing step
          - name: ocr-processing
            template: model-inference
            dependencies: [initialize]
            arguments:
              parameters:
                - name: model-name
                  value: "ocr-model"
                - name: model-endpoint
                  value: "http://ocr-model-service:8000"
                - name: input-data
                  value: "{{tasks.initialize.outputs.parameters.stage-0-data}}"
                - name: input-field
                  value: "image"
                - name: output-field
                  value: "text"
          
          # NER processing step
          - name: ner-processing
            template: model-inference
            dependencies: [ocr-processing]
            arguments:
              parameters:
                - name: model-name
                  value: "bert-ner"
                - name: model-endpoint
                  value: "http://bert-ner-model-service:8000"
                - name: input-data
                  value: "{{tasks.ocr-processing.outputs.parameters.result}}"
                - name: input-field
                  value: "text"
                - name: output-field
                  value: "entities"
          
          # Finalize pipeline
          - name: finalize
            template: finalize-pipeline
            dependencies: [ner-processing]
            arguments:
              parameters:
                - name: final-result
                  value: "{{tasks.ner-processing.outputs.parameters.result}}"
                - name: pipeline-id
                  value: "{{workflow.name}}"
    
    # Initialize pipeline
    - name: initialize-pipeline
      inputs:
        parameters:
          - name: input-data
          - name: pipeline-config
      outputs:
        parameters:
          - name: stage-0-data
            valueFrom:
              path: /tmp/stage-0-data.json
          - name: pipeline-id
            valueFrom:
              path: /tmp/pipeline-id.txt
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            import uuid
            import os
            
            # Generate pipeline ID
            pipeline_id = str(uuid.uuid4())
            with open('/tmp/pipeline-id.txt', 'w') as f:
                f.write(pipeline_id)
            
            # Parse input data
            input_data = json.loads('''{{inputs.parameters.input-data}}''')
            pipeline_config = json.loads('''{{inputs.parameters.pipeline-config}}''')
            
            # Prepare initial stage data
            stage_data = {
                "pipeline_id": pipeline_id,
                "stage": 0,
                "data": input_data,
                "metadata": {
                    "timestamp": "$(date -Iseconds)",
                    "pipeline_config": pipeline_config
                }
            }
            
            with open('/tmp/stage-0-data.json', 'w') as f:
                json.dump(stage_data, f)
            
            print(f"Pipeline {pipeline_id} initialized")
        volumeMounts:
          - name: pipeline-data
            mountPath: /tmp/pipeline-data
    
    # Generic model inference template
    - name: model-inference
      inputs:
        parameters:
          - name: model-name
          - name: model-endpoint
          - name: input-data
          - name: input-field
          - name: output-field
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result.json
          - name: metrics
            valueFrom:
              path: /tmp/metrics.json
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            import requests
            import time
            import sys
            
            # Parse inputs
            model_name = "{{inputs.parameters.model-name}}"
            model_endpoint = "{{inputs.parameters.model-endpoint}}"
            input_data = json.loads('''{{inputs.parameters.input-data}}''')
            input_field = "{{inputs.parameters.input-field}}"
            output_field = "{{inputs.parameters.output-field}}"
            
            print(f"Processing with {model_name}")
            print(f"Endpoint: {model_endpoint}")
            
            try:
                # Extract input for this model
                if "data" in input_data:
                    model_input = input_data["data"]
                else:
                    model_input = input_data
                
                # Prepare request
                request_data = {"data": {input_field: model_input.get(input_field, model_input)}}
                
                # Make inference request
                start_time = time.time()
                response = requests.post(
                    f"{model_endpoint}/predict",
                    json=request_data,
                    headers={"Content-Type": "application/json"},
                    timeout=60
                )
                inference_time = time.time() - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Extract output
                    if "predictions" in result:
                        output_data = result["predictions"]
                    else:
                        output_data = result
                    
                    # Prepare result for next stage
                    stage_result = {
                        "pipeline_id": input_data.get("pipeline_id"),
                        "stage": input_data.get("stage", 0) + 1,
                        "data": {output_field: output_data},
                        "metadata": {
                            "model_name": model_name,
                            "inference_time": inference_time,
                            "timestamp": time.time()
                        }
                    }
                    
                    # Save result
                    with open('/tmp/result.json', 'w') as f:
                        json.dump(stage_result, f)
                    
                    # Save metrics
                    metrics = {
                        "model_name": model_name,
                        "inference_time": inference_time,
                        "status": "success",
                        "timestamp": time.time()
                    }
                    with open('/tmp/metrics.json', 'w') as f:
                        json.dump(metrics, f)
                    
                    print(f"Inference completed in {inference_time:.3f}s")
                    
                else:
                    error_msg = f"Model inference failed: {response.status_code} {response.text}"
                    print(error_msg)
                    
                    # Save error result
                    error_result = {
                        "pipeline_id": input_data.get("pipeline_id"),
                        "stage": input_data.get("stage", 0) + 1,
                        "error": error_msg,
                        "metadata": {
                            "model_name": model_name,
                            "inference_time": inference_time,
                            "timestamp": time.time()
                        }
                    }
                    with open('/tmp/result.json', 'w') as f:
                        json.dump(error_result, f)
                    
                    sys.exit(1)
                    
            except Exception as e:
                error_msg = f"Exception during inference: {str(e)}"
                print(error_msg)
                
                # Save error result
                error_result = {
                    "pipeline_id": input_data.get("pipeline_id"),
                    "error": error_msg,
                    "metadata": {
                        "model_name": model_name,
                        "timestamp": time.time()
                    }
                }
                with open('/tmp/result.json', 'w') as f:
                    json.dump(error_result, f)
                
                sys.exit(1)
        volumeMounts:
          - name: pipeline-data
            mountPath: /tmp/pipeline-data
    
    # Finalize pipeline
    - name: finalize-pipeline
      inputs:
        parameters:
          - name: final-result
          - name: pipeline-id
      outputs:
        parameters:
          - name: pipeline-result
            valueFrom:
              path: /tmp/pipeline-result.json
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            import time
            
            # Parse final result
            final_result = json.loads('''{{inputs.parameters.final-result}}''')
            pipeline_id = "{{inputs.parameters.pipeline-id}}"
            
            # Create final pipeline result
            pipeline_result = {
                "pipeline_id": pipeline_id,
                "status": "completed",
                "result": final_result.get("data", final_result),
                "metadata": {
                    "completion_time": time.time(),
                    "total_stages": final_result.get("stage", 1)
                }
            }
            
            with open('/tmp/pipeline-result.json', 'w') as f:
                json.dump(pipeline_result, f)
            
            print(f"Pipeline {pipeline_id} completed successfully")
            print(f"Final result: {json.dumps(pipeline_result, indent=2)}")
        volumeMounts:
          - name: pipeline-data
            mountPath: /tmp/pipeline-data

---
# Workflow for batch processing
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: batch-multi-model-pipeline
  namespace: ai-models
  labels:
    app: pipeline-orchestrator
spec:
  entrypoint: batch-pipeline
  
  arguments:
    parameters:
      - name: batch-data
        description: "Array of input data for batch processing"
      - name: pipeline-config
        description: "Pipeline configuration JSON"
      - name: batch-size
        value: "10"
        description: "Number of items to process in parallel"
  
  templates:
    - name: batch-pipeline
      inputs:
        parameters:
          - name: batch-data
          - name: pipeline-config
          - name: batch-size
      steps:
        # Split batch into chunks
        - - name: split-batch
            template: split-batch
            arguments:
              parameters:
                - name: batch-data
                  value: "{{inputs.parameters.batch-data}}"
                - name: batch-size
                  value: "{{inputs.parameters.batch-size}}"
        
        # Process each chunk in parallel
        - - name: process-chunk
            template: process-chunk
            arguments:
              parameters:
                - name: chunk-data
                  value: "{{item}}"
                - name: pipeline-config
                  value: "{{inputs.parameters.pipeline-config}}"
            withParam: "{{steps.split-batch.outputs.parameters.chunks}}"
        
        # Combine results
        - - name: combine-results
            template: combine-results
            arguments:
              parameters:
                - name: chunk-results
                  value: "{{steps.process-chunk.outputs.parameters.chunk-result}}"
    
    - name: split-batch
      inputs:
        parameters:
          - name: batch-data
          - name: batch-size
      outputs:
        parameters:
          - name: chunks
            valueFrom:
              path: /tmp/chunks.json
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            
            batch_data = json.loads('''{{inputs.parameters.batch-data}}''')
            batch_size = int("{{inputs.parameters.batch-size}}")
            
            # Split into chunks
            chunks = []
            for i in range(0, len(batch_data), batch_size):
                chunk = batch_data[i:i + batch_size]
                chunks.append(chunk)
            
            with open('/tmp/chunks.json', 'w') as f:
                json.dump(chunks, f)
            
            print(f"Split {len(batch_data)} items into {len(chunks)} chunks")
    
    - name: process-chunk
      inputs:
        parameters:
          - name: chunk-data
          - name: pipeline-config
      outputs:
        parameters:
          - name: chunk-result
            valueFrom:
              path: /tmp/chunk-result.json
      steps:
        - - name: process-items
            template: multi-model-pipeline
            arguments:
              parameters:
                - name: input-data
                  value: "{{item}}"
                - name: pipeline-config
                  value: "{{inputs.parameters.pipeline-config}}"
            withParam: "{{inputs.parameters.chunk-data}}"
    
    - name: combine-results
      inputs:
        parameters:
          - name: chunk-results
      outputs:
        parameters:
          - name: final-result
            valueFrom:
              path: /tmp/final-result.json
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            
            # Combine all chunk results
            chunk_results = json.loads('''{{inputs.parameters.chunk-results}}''')
            
            final_result = {
                "status": "completed",
                "total_items": len(chunk_results),
                "results": chunk_results
            }
            
            with open('/tmp/final-result.json', 'w') as f:
                json.dump(final_result, f)
            
            print(f"Combined {len(chunk_results)} chunk results")

